[{"content":"文章简介 本文借鉴了心里学中的priming effect，类似长短时程记忆的概念即人脑会对提前收到的刺激有更快的响应，提出了SMTM（Semantic Memory）的概念，用于加速神经网络的运算。SMTM加速神经网络的理念为：将神经网络每一层的输出与SMTM中的记忆进行对比，若具有足够的可信度确认类别，则直接输出结果，略过剩余层数。在SMTM的具体实时过程中存在三个主要问题：\nCache中查找内存耗时过长，以至于时间开销比完整地运行神经网络更大。 现存的神经网络结构中不支持早停的功能 同一类数据的的持续时间可能不一样，会带来不同的类型的刺激 文章从这三个问题出发，提出了SMTM的缓存概念，应用三种方法加速Cache的查找并提出了一个SMTM应用于CNN的demo。并通过SMTM应用于VGG ALEXNET等五种神经网络后的延迟降低和准确率下降展示了SMTM的强大性能。\n有趣的点 看似文章是提出了一个全新的东西，但是只是融合了几个旧概念并进行改良。 有研究者实现过利用缓存存储神经网络中间量用于分类任务（文中参考文献5，14），也有研究者实现过提前退出的神经网络（文中参考文件12，25，27，41，42，45，46），作者在这个基础上提出了semantic vector用于减少查找缓存的消耗，并结合早停和中间量查找实现CNN加速。\nSMTM应用于神经网络后应该怎么训练\nsemantic vector是如何提取的\n基于semantic vector的分类和置信度是如何计算的\nsemantic vector的聚类中心的更新法则是如何确定的\nSMTM整体工作流程 SMTM作为一种额外的缓存机制外挂于现存的神经网络中，通过早停的方式对神经网络计算进行加速，其具体拓扑结构如图所示 SMTM从神经网络的每一层输出中提取Semantic Vector，并与缓存中的Semantic Vector对比得出识别结果和置信度。若置信度足够则直接退出神经网络得出结果，若不够则继续下一层神经网路的计算，从而实现神经网络的加速功能。\nsemantic vector的对比也需要着计算量和延迟开销。过大的缓存会导致更长的延迟，甚至反而降低神经网路的计算速度。为此SMTM将记忆分为Fast memory和Global memory两部分，在Fast memory中存放最近多次出现的热点种类，在global memory存放完整的种类。SMTM工作时仅对比Fast memory中的类别，并通过数学方法更新fast memory中的内容和大小，以微弱的性能代价，大幅降低了缓存对比的工作量。\nSemantic Vector的计算 SMTM的识别依赖于向量空间中semantic vector之间的距离，在这一判断过程中存在大量的查找和对比计算。因此semantic vector既要含有识别物体的特征信息，又要压缩大小。为此，本文选择了GAP（global average pooling）进行semantic vector的提取。 如图所示，直接采用神经网络层中的的特征信息会导致过大的semantic vector，因此本文对神经网络的每一层进行GAP处理，得到一个小规模的特征向量。具体为将(C，H，W)的图片特征池化为（C，1）的特征向量，其中C为channel层通道数，H，W为特征图的高度和宽度。\n如何利用Semantic Vector进行分类 利用Semantic Vector进行分类本质上是一个聚类任务。总的来说，将神经网络输出所提取的Semantic Vector作为分类对象，计算其与所有的标签的类别中心的距离，从而得出识别结果和置信度。其中涉及到了三个问题：\nSemantic Vector空间的距离计算 聚类中心如何获取 置信度如何计算 Semantic Vector空间的距离计算 聚类任务中，类内距离和类间距离是表征向量分类效果的指标。当类内距离很小类间距离很大时，就能够区分一个向量的类别。semantic vector总体来说依然是一个高维向量（C可能高达几百），因此本文采用了t-SNE的方式对其进行降维处理以实现可视化，如图所示。 可以看到，随着神经网络的层数递增，不同的semantic vector之间的区分度越来越大，说明更好分离。\n在实际的运算中本文采用了cosine距离来进行计算。具体而言，令第l层的semantic vector为$SV^l$，相应标签j的聚类中心为$SC_j^l$，则二者之间的距离为： $$ s_j^l=\\xi(SV^l,SC^l_j) $$ 其中$\\xi()$为cosine相似函数(余弦相似度)，只计算两个向量之间的夹角，而忽略向量长度的差异即 $$ \\xi(\\vec{x},\\vec{y}) =\\frac{\\vec{x}\\cdot \\vec{y}}{\\vert x\\vert\\vert y\\vert} $$置信度的计算方式 置信度表征着semantic vector是否被正确的归类，取决于向量与最近的聚类中心距离和其他聚类中心距离。本文的置信度通过向量与最近和第二近聚类中心距离得出，如下式： $$ sep = \\frac{s^l_H-s^l_{SH}}{s^l_{SH}} $$ 向量离最近聚类中心越近，第二聚类中心越远则说明越可信。\n同时考虑到神经网络具有很多层，文章提出了跨层累积置信度的概念，不仅考虑当前层的置信度，还考虑了前面层数的置信度。累积置信度为 $$ SA^l_j=\\sum_{l_0=1}^l s_j^{l_0}\\times W_{l_0} $$ 其中$W_{l_0}=2^{l_0-1}$为每层的权重，确保了每次执行都的计算中当前层和之前所有层的占比约为一半。\n聚类中心更新法则 由于测试数据集和实际应用场景存在差异，测试集中的聚类中心可能不适用于实际应用。此外，不同的应用场景会对聚类中心产生不同程度的偏移。因此，需要动态地更新聚类中心，以适应真实世界多变的数据。\n聚类中心的更新服从公式： $$ {SC^j_{l_0}}^{'}=\\frac{SC_{l_0}^j \\cdot m_{l_0}^j+SV_{l_0}^j}{m_{l_0}^j+1} $$ 其中，$j$代表类别，$l_0$代表层数，$SC^j_{l_0}$代表原本的聚类中心的位置，${SC^j_{l_0}}^{\u0026rsquo;}$代表更新后的聚类中心位置。$SV_{l_0}^j$是测试时新样本的semantic vector，$m_{l_0}^j$是总共的更新次数，包括训练时的更新次数和应用时的更新次数。该递推式意味着，聚类中心为训练和测试中所有semantic vector的平均值，确保了聚类中心随着新生成的semantic vector进行移动，具有适应不同环境的能力。\n缓存大小决定策略 为了保证高效的查找，fast memory中只能保存有限的聚类中心，而不同的场景中不同的类别的物体出现的频率不同，为此需要动态地跟新fast memory的缓存。\nfast memory需要保存最新，最频繁出现的向量，为此本文提出了frequency table 和time-stamp table用于检测所有的类别出现的时间和频率，并结合这两项指标得出一个综合分数，依据综合分数对所有的类别继续排序，决定fast memory中储存内容。\nfrequency table：记录所有标签的总出现次数，用$FT_i$表示 time-stamp table：记录所有标签未出现的帧数，用$TS_i$表示 综合分数依照下式进行计算 $$ Score_i = FT_i \\times (0.25)^{[\\frac{TS_i}{W}]} $$ 某个标签总出现次数越多，则基础的得分越高。同时还受出现时间的影响，越久没有出现则衰减的越多。\n缓存具有动态的大小，论文中的思想是动态调整缓存大小，使得内部包含标签占据总分的95%，满足公式： $$ P(A)=\\sum_{i=1}^k \\frac{Score_i}{\\sum_{i=1}^n Score_j} $$ P(A)为缓存中标签的分数和占据总分的百分比，论文参考前人的论文将P(A)定为0.95\n","permalink":"https://ttttwwww.github.io/zh/posts/paper-reading-notes/boosting-mobile-cnn-inference-through-semantic-memory/","summary":"文章简介 本文借鉴了心里学中的priming effect，类似长短时程记忆的概念即人脑会对提前收到的刺激有更快的响应，提出了SMTM（Sema","title":"Boosting Mobile CNN Inference Through Semantic Memory"},{"content":"t-SNE t-SNE全称为t-distributed Stochastic Neighbor Embedding，t随机邻近嵌入。是效果最好的数据降维和可视化方法之一，用于将高维数据集投影到低维，用于判断数据集有没有很好的可分性。若低维投影可分则源数据集一定可分，若低维投影不可分不一定说明源数据集不可分。\n从该文阅读：https://zhuanlan.zhihu.com/p/426068503\nt-SNE的基本思想是，原本在高维空间中距离远的数据在投影空间后的距离也应该很远，同样的，原本距离近的数据降维后也应该很近。为了实现这一点，将距离的远近关系转换为概率分布。这样降维前后的数据都将对应一个概率分布，若两个概率分布接近，则两者的距离关系也类似。 实际操作时，会在低维空间中随机生成相应数量的数据点，然后调整其中数据点的位置来使得低维空间和高维空间之间的概率分布接近。\n描述远近关系的分布概率为： $$ p(j|i) = \\frac{S(x_i,x_j)}{\\sum_{k\\neq i}S(x_i,x_k)}\\quad,j\\neq i,i=1,2,\\dots,n $$其中$S(x_i,x_j)$是描述数据点$i,j$之间的相似度的函数。在高维空间中，SNE和t-SNE均采用欧式距离指数衰减的形式： $$ S(x_i,x_j)=\\exp(-\\Vert x_i-x_j \\Vert_2^2/(2\\sigma_i^2)) $$ 高维空间中两个数据点的距离来开，相似度将迅速减小，增大低维空间中数据点之间的距离。方差项的引入让其更像一个正态分布？（来自知乎作者的说法，不知道为什么）其中针对每一个数据点i的方差值都不一样，通常会引入一个困惑度的概念来决定方差。 $$ Perp(P_i)=2^{H(P_i)}\\\\ H(P_i)=-\\sum_{j\\neq i}p(j|i)\\log_{2}{p(j|i)} $$ 其中$H(P_i)$为信息熵，表达分布的复杂程度，H越大分布越复杂，当事件分布越均匀时H越大。这是一个约束条件下的极值问题，可以用拉格朗日乘数法求解。随着方差$\\sigma_i$的增大H单调增大（没有手动证明，来自知乎），因此针对每一个点i可以用二分法找到一个困惑度合理的$\\sigma_i$。\n在低维空间中，SNE算法依然采用高维空间的形式，只是指定了方差为$\\frac{1}{\\sqrt{2}}$。而t-SNE算法采用如下的相似度定义： $$ S^{'}(z_i,z_j)=[1+\\Vert z_i-z_j \\Vert]^{-1} $$ 如图所示，通过新的相似度函数，使得原本相近的点离得更近，更远的点里的更远，从而解决了拥堵问题。 上述工作完成了高维空间和低维空间的概率分布的定义，接下来需要定义两个分布的差异，通常采用KL散度来定义二者的差异。 $$ D_{KL}(p||q)=\\sum_x P(x)log\\frac{P(x)}{Q(x)}=\\sum_x P(x)(logP(x)-logQ(x)) $$ KL散度表示两种分布之间的信息损失，KL散度越小代表两种分布越相似。\n因此t-SNE需要调整随机生成的低维空间的数据以降低两个分布之间的KL散度。即优化任务描述为 $$ L(z_1,z_2,\\dots,z_n=\\sum_{i=1}^n\\sum_{j\\neq i} P(j|i)\\log \\frac{P(j|i)}{Q(j|i)})\\\\ (z_1^*,z_2^*,\\dots,z_n^*)=arg min L $$ 余下的部分和神经网络的更新类似，KL散度作为损失函数，然后进行梯度下降更新直到收敛。\n","permalink":"https://ttttwwww.github.io/zh/posts/math-algorithm/t-sne/","summary":"t-SNE t-SNE全称为t-distributed Stochastic Neighbor Embedding，t随机邻近嵌入。是效果最好的数据降维和可视化方法之一，用于将高维数据集投","title":"T SNE"},{"content":"问题背景 在自制储备池计算数据集时，需要计算储池器件在电压序列下的响应。计算过程为逐时间步模拟储池器件的内部电导态在输入电压激励下的响应，因此，计算耗时随着时间步增多而增长。该实验需要计算5种储池器件对于16种不同时间精度的电压序列的响应，其中每个电压序列含有2000时间步。若采用串行计算的方式，进行一次计算耗费近两个小时时间。为了加快计算速度，采用并行计算的方式，同时模拟多组器件的响应过程。\n技术方案 多线程和多进程 该实验采用python代码编写。由于python存在GIL(全局解释器锁)，同一时刻只能有一个线程执行python字节码，多线程并不能很好的利用多核cpu加速运算。为此，采用python的多进程编程.在python的运行过程中创建多个子进程，每个子进程拥有自身的解释器，从而绕开GIL的限制，充分利用多核进行加速运算。\n多进程代码 pool管理多进程时子进程不工作 queue创建问题 在采用queue传输数据时，传递给子进程的queue需要由mp.Manager创建，否则子进程将不执行。\n1 2 3 4 5 6 7 queue = mp.Queue() pool = mp.Pool(processes=100) for i in range(number_plot): pool.apply_async(func=HandGestureDataset._preview_multi_plot, args=(dataset.datas[i].reshape(32, 32, ), dataset.labels[i], queue,)) pool.close() pool.join() 需要将queue替换为manager创建的queue子进程才能运行\n1 2 3 4 5 6 7 8 manager = mp.Manager() queue = manager.Queue() pool = mp.Pool(processes=100) for i in range(number_plot): pool.apply_async(func=HandGestureDataset._preview_multi_plot, args=(dataset.datas[i].reshape(32, 32, ), dataset.labels[i], queue,)) pool.close() pool.join() 由chatgpt解答：\nmp.Queue和mp.Manager.Queue都能创建队列。queue直接依赖操作系统提供的管道(pipe)和信号(semaphore)机制，因此只能在父进程和其直接派生的子进程之间通信。然而pool时父进程创建的池子，不是直接由父进程派生的，因此父进程与pool之间的子进程无法直接通信。manager.queue创建的queue通过一个服务器进程管理，所有的子进程均可通过代理访问，从而实现子进程之间的正常通信，无关子进程的创建方式。 ","permalink":"https://ttttwwww.github.io/zh/posts/python/pool.apply_async%E5%AD%90%E8%BF%9B%E7%A8%8B%E4%B8%8D%E6%89%A7%E8%A1%8C/","summary":"问题背景 在自制储备池计算数据集时，需要计算储池器件在电压序列下的响应。计算过程为逐时间步模拟储池器件的内部电导态在输入电压激励下的响应，因此","title":"Python多进程编程"},{"content":"在神经网络的训练中会采用各样的损失函数以衡量预测值和真实值之间的偏离程度，交叉熵是其中一种广泛应用于分类任务的损失函数。\n交叉熵的定义公式为 $$ H = -\\sum_{i}^n P(i)\\log Q(i) $$ 底数为log代表底数任取，在信息论中依照惯例取2，为了简化求导计算可以取e。\n其中P为事件的真实分布概率，Q为预测事件概率。当预测的分布概率于真实的分布概率越接近时，交叉熵的取值就越小。因此往往采用交叉熵作为分类任务的损失函数。\n在神经网络的最后一层输出通常是各个神经元的输出值，一般采用softmax函数等方式将其转化为均等的概率。在pytorch的代码中，计算交叉熵之前会对输入值自动进行softmax计算，无需额外增加一层softmax\n在pytorch的具体实现中，采用\n1 2 3 4 # 定义交叉熵损失函数 criterion = nn.CrossEntropyLoss() # 计算损失 loss = criterion(inputs, targets) 实际训练时的样本通常是一个数据集对应一个标签，而不是一个概率分布。因此最终的P(i)通常是二值的，即只有0和1，此时的交叉熵公式转变为 $$ H = -\\log Q(i) $$ 而Q一般由softmax函数得来，公式可以改写为 $$ H = -\\log \\frac{\\exp {y(i)}}{ \\sum\\limits_{j} \\exp y(j)} $$ 此处的y代表着最后一层神经元的输出值。在进行反向传播时对于一个如图所示的神经网络 z层代表最终的概率值，y为交叉熵公式中的y。假定此时的真实样本为yk，其他的标签的交叉熵导数为0，则反向传播将从zk开始。 首先考虑最后一层zk对y的影响，该层的影响为交叉熵对所有y层神经元输出求偏导有(为简化计算，log底数取e)\n$$ \\frac{\\partial H}{\\partial y_i} = \\frac{\\partial z_k}{\\partial y_i} = \\left\\{ \\begin{matrix} \\frac{\\exp(y_i)}{\\sum\\limits_n \\exp (y_i)}-1(i==k)\\\\ \\frac{\\exp(y_i)}{\\sum\\limits_n \\exp (y_i)} (i\\neq k) \\end{matrix} \\right. $$更新权重为负梯度，可以看到从yk处向后传的梯度是负数，其他神经元向后传播的梯度为正数。因此，yk前级的权重连接将得到增强，其他神经元的前级连接将被削弱。然后再看前级的梯度传播，首先考虑h层到y层的连接权重，此时的更新公式为 $$ \\frac{\\partial H}{\\partial W_{jk}}=\\frac{\\partial H}{\\partial y_k}*\\frac{\\partial y_k}{\\partial W_{jk}}=\\frac{\\partial H}{\\partial y_k}\\\\ y_k=f(W_{jk}*h_j+b) $$ 对于传统的ANN，$\\frac{\\partial y_k}{\\partial W_{jk}}=h_j*\\frac{df}{dh_j}$，f为激活函数，即此时变更权重的大小取决于hk神经元此时的发放值。神经元传输变量越大，则认为二者相关，会有较强的调整，但是式中还有激活函数的影响。例如sigmoid函数的导数会在输入值很大时趋于0，此时权重将几乎不会进行调整，并且可能导致梯度消失的问题，若采用relu函数，则当输入很大时，导数恒定为常数，权重将继续按照前级神经元的输出进行更新。\n在论文A neuromorphic physiological signal processing system based on VO2 memristor for next-generation human-machine interface中，其神经网络结构为 中间两层可以视为Hidden层。该神经网络为SNN，采用梯度替代的方式进行反向传播训练。神经元的发放函数的替代导数采用如下公式\n该公式产生的影响和sigmoid函数类似，当原本的权重过大或着过小时，权重的调节就会变得很缓慢，对于浅层的神经网络，只要权重不过大也许还是能脱出？\n","permalink":"https://ttttwwww.github.io/zh/posts/ann/%E4%BA%A4%E5%8F%89%E7%86%B5%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/","summary":"在神经网络的训练中会采用各样的损失函数以衡量预测值和真实值之间的偏离程度，交叉熵是其中一种广泛应用于分类任务的损失函数。 交叉熵的定义公式为 $$","title":"交叉熵反向传播"},{"content":"Resume算法是Filip Ponulak等人提出的一种用于脉冲神经网络监督学习的权重更新算法，主要针对仿生神经元。 使用Resume算法训练后的仿生神经元，能够学习模拟任意脉冲序列，并基于脉冲的时间配置，对不同的脉冲序列分类。\n算法细节 神经网络的训练体现在权重的调节上，Resume算法是一种指导类型的算法，通过给定的脉冲序列以指导神经网络中权重的调节，从而使得神经网络有着相同的脉冲发放行为。\n具体而言，神经网络中权重的调节依赖于输入脉冲$x_i$，指导脉冲$y_d$，和真实脉冲$y_o$。 $$\r\\Delta \\omega_{oi}=x_iy_d-x_iy_o\\\\\r$$ 其中，输入脉冲是神经网络自身接收到的输入脉冲信号，指导脉冲是目标神经网络在输入脉冲刺激下产生的脉冲响应，真实脉冲则是当前神经网络在输入脉冲刺激下产生的脉冲响应。\n该过程可以类比STDP过程和反STDP过程。右侧表达式的第一项认为，该输入脉冲和发放的脉冲有关联，从而增强连接。第二项则认为，该输入脉冲于发放脉冲无关联，从而削弱脉冲。\n表达式对时间求导 $$\r\\frac{d\\omega_{oi}}{dx_i}=X_{di}-X_{oi}\\\\\r$$","permalink":"https://ttttwwww.github.io/zh/posts/snn/resume/resume%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC/","summary":"Resume算法是Filip Ponulak等人提出的一种用于脉冲神经网络监督学习的权重更新算法，主要针对仿生神经元。 使用Resume算法训练","title":"Resumse算法推导"}]